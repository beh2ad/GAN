{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "484abbd3",
   "metadata": {},
   "source": [
    "Write a code using the DCGAN architecture that is able to generate images similar to handwritten digits in the MNIST dataset format. Considering that this architecture uses CNN networks, include the following items in the report:\n",
    "\n",
    "(a) The topology of the generator and discriminator layers in tensor form \\\n",
    "(b) The way the loss functions and activation functions work (along with the reason for using them) \\\n",
    "(c) Batch normalization (how it works and its importance) \\\n",
    "(d) The need for a dropout layer \\ \n",
    "(e) The method of generating noise \\\n",
    "(f) A complete explanation of how the ADAM optimizer works \\\n",
    "(g) Loss and accuracy plots for both the generator and the discriminator \\\n",
    "(h) Sample outputs of the generator network at several specific epochs, along with justification of the improvement in the networkâ€™s performance over time \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7ff9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam\n",
    "from tqdm import tqdm\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "%matplotlib inline\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "\n",
    "# Scaling the range of the image to [-1, 1]\n",
    "# Because we are using tanh as the activation function in the last layer of the generator\n",
    "# and tanh restricts the weights in the range [-1, 1]\n",
    "X_train = (X_train - 127.5) / 127.5\n",
    "\n",
    "X_train.shape\n",
    "\n",
    "generator = Sequential()\n",
    "generator.add(Dense(128*7*7, input_dim=100, activation=LeakyReLU(0.2)))\n",
    "generator.add(BatchNormalization())\n",
    "generator.add(Reshape((7,7,128)))\n",
    "generator.add(Conv2DTranspose(64, kernel_size=5,strides=2,padding='same', activation=LeakyReLU(0.2)))\n",
    "generator.add(BatchNormalization())\n",
    "generator.add(Conv2DTranspose(1, kernel_size=5,strides=2, padding=\"same\", activation=\"tanh\"))\n",
    "\n",
    "print('generator_model')\n",
    "generator.summary()\n",
    "\n",
    "discriminator = Sequential()\n",
    "discriminator.add(Convolution2D(64, kernel_size=5,strides=2, input_shape=(28,28,1), padding=\"same\", activation=LeakyReLU(0.2)))\n",
    "discriminator.add(Dropout(0.3))\n",
    "discriminator.add(Convolution2D(128, kernel_size=5,strides=2, padding=\"same\", activation=LeakyReLU(0.2)))\n",
    "discriminator.add(Dropout(0.3))\n",
    "discriminator.add(Flatten())\n",
    "discriminator.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print('discriminator_model')\n",
    "discriminator.summary()\n",
    "\n",
    "generator.compile(loss='binary_crossentropy', optimizer=Adam(),metrics=['accuracy'])\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(),metrics=['accuracy'])\n",
    "\n",
    "discriminator.trainable = False\n",
    "ganInput = Input(shape=(100,))\n",
    "# getting the output of the generator\n",
    "# and then feeding it to the discriminator\n",
    "# new model = D(G(input))\n",
    "x = generator(ganInput)\n",
    "ganOutput = discriminator(x)\n",
    "gan = Model(input=ganInput, output=ganOutput)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=Adam(),metrics=['accuracy'])\n",
    "print('GAN_model')\n",
    "gan.summary()\n",
    "\n",
    "\n",
    "def train(epoch=10, batch_size=128):\n",
    "    \n",
    "    D_loss=np.zeros((epoch,1))\n",
    "    D_acc=np.zeros((epoch,1))\n",
    "    G_loss=np.zeros((epoch,1))\n",
    "    G_acc=np.zeros((epoch,1))\n",
    "    x=np.arange(0,epoch)\n",
    "    \n",
    "    for i in range(epoch):\n",
    "      \n",
    "      if math.floor(i/50)==(i/50):\n",
    "        print('epoch={}'.format(i))\n",
    "      \n",
    "      for j in range(1):  \n",
    "        # Input for the generator\n",
    "        noise_input = np.random.rand(batch_size, 100)\n",
    "\n",
    "        # getting random images from X_train of size=batch_size \n",
    "              # these are the real images that will be fed to the discriminator\n",
    "        image_batch = X_train[np.random.randint(0, X_train.shape[0], size=batch_size)]\n",
    "\n",
    "              # these are the predicted images from the generator\n",
    "        predictions = generator.predict(noise_input, batch_size=batch_size)\n",
    "\n",
    "              # the discriminator takes in the real images and the generated images\n",
    "        X = np.concatenate([predictions, image_batch])\n",
    "\n",
    "              # labels for the discriminator\n",
    "        y_discriminator = [0]*batch_size + [1]*batch_size\n",
    "\n",
    "              # Let's train the discriminator\n",
    "        discriminator.trainable = True\n",
    "        D=discriminator.fit(X, y_discriminator,verbose=0)\n",
    "      \n",
    "      for k in range(3):\n",
    "              # Let's train the generator\n",
    "        noise_input = np.random.rand(batch_size, 100)\n",
    "        y_generator = [1]*batch_size\n",
    "        discriminator.trainable = False\n",
    "        G=gan.fit(noise_input, y_generator,verbose=0)\n",
    "      \n",
    "      D_loss[i]= D.history['loss']\n",
    "      G_loss[i]= G.history['loss']\n",
    "      D_acc[i]= D.history['acc']\n",
    "      G_acc[i]= G.history['acc']\n",
    "\n",
    "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "    # summarize history for accuracy\n",
    "    axs[0].plot(x,D_acc)\n",
    "    axs[0].plot(x,G_acc)\n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].legend(['discriminator', 'generator'], loc='best')\n",
    "    # summarize history for loss\n",
    "    axs[1].plot(x,D_loss)\n",
    "    axs[1].plot(x,G_loss)\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].legend(['discriminator', 'generator'], loc='best')\n",
    "    plt.show()  \n",
    "\n",
    "train(3000, 128)\n",
    "\n",
    "def plot_output():\n",
    "    try_input = np.random.rand(100, 100)\n",
    "    preds = generator.predict(try_input)\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(preds.shape[0]):\n",
    "        plt.subplot(10, 10, i+1)\n",
    "        plt.imshow(preds[i, :, :, 0], cmap='gray')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    # tight_layout minimizes the overlap between 2 sub-plots\n",
    "    plt.tight_layout()\n",
    "  \n",
    "plot_output()\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
