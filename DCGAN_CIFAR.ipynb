{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a015cae6",
   "metadata": {},
   "source": [
    "Write a code using the DCGAN architecture that is able to generate images similar to multi-calss images in the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c9f6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, Reshape, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation, Conv2D, Conv2DTranspose\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "def get_generator(input_layer, condition_layer):\n",
    "\n",
    "  merged_input = Concatenate()([input_layer, condition_layer])\n",
    "  \n",
    "  hid = Dense(128 * 8 * 8, activation='relu')(merged_input)    \n",
    "  hid = BatchNormalization(momentum=0.9)(hid)\n",
    "  hid = LeakyReLU(alpha=0.1)(hid)\n",
    "  hid = Reshape((8, 8, 128))(hid)\n",
    "\n",
    "  hid = Conv2D(128, kernel_size=4, strides=1,padding='same')(hid)\n",
    "  hid = BatchNormalization(momentum=0.9)(hid)    \n",
    "  hid = LeakyReLU(alpha=0.1)(hid)\n",
    "\n",
    "  hid = Conv2DTranspose(128, 4, strides=2, padding='same')(hid)\n",
    "  hid = BatchNormalization(momentum=0.9)(hid)\n",
    "  hid = LeakyReLU(alpha=0.1)(hid)\n",
    "\n",
    "  hid = Conv2D(128, kernel_size=5, strides=1,padding='same')(hid)\n",
    "  hid = BatchNormalization(momentum=0.9)(hid)    \n",
    "  hid = LeakyReLU(alpha=0.1)(hid)\n",
    "\n",
    "  hid = Conv2DTranspose(128, 4, strides=2, padding='same')(hid)\n",
    "  hid = BatchNormalization(momentum=0.9)(hid)\n",
    "  hid = LeakyReLU(alpha=0.1)(hid)\n",
    "\n",
    "  hid = Conv2D(128, kernel_size=5, strides=1, padding='same')(hid)\n",
    "  hid = BatchNormalization(momentum=0.9)(hid)\n",
    "  hid = LeakyReLU(alpha=0.1)(hid)\n",
    "\n",
    "  hid = Conv2D(128, kernel_size=5, strides=1, padding='same')(hid)\n",
    "  hid = BatchNormalization(momentum=0.9)(hid)\n",
    "  hid = LeakyReLU(alpha=0.1)(hid)\n",
    "                      \n",
    "  hid = Conv2D(3, kernel_size=5, strides=1, padding=\"same\")(hid)\n",
    "  out = Activation(\"tanh\")(hid)\n",
    "\n",
    "  model = Model(inputs=[input_layer, condition_layer], outputs=out)\n",
    "  print('generator_model')\n",
    "  model.summary()\n",
    "  \n",
    "  return model, out\n",
    "\n",
    "def get_discriminator(input_layer, condition_layer):\n",
    "  hid = Conv2D(128, kernel_size=3, strides=1, padding='same')(input_layer)\n",
    "  hid = BatchNormalization(momentum=0.9)(hid)\n",
    "  hid = LeakyReLU(alpha=0.1)(hid)\n",
    "\n",
    "  hid = Conv2D(128, kernel_size=4, strides=2, padding='same')(hid)\n",
    "  hid = BatchNormalization(momentum=0.9)(hid)\n",
    "  hid = LeakyReLU(alpha=0.1)(hid)\n",
    "\n",
    "  hid = Conv2D(128, kernel_size=4, strides=2, padding='same')(hid)\n",
    "  hid = BatchNormalization(momentum=0.9)(hid)\n",
    "  hid = LeakyReLU(alpha=0.1)(hid)\n",
    "\n",
    "  hid = Conv2D(128, kernel_size=4, strides=2, padding='same')(hid)\n",
    "  hid = BatchNormalization(momentum=0.9)(hid)\n",
    "  hid = LeakyReLU(alpha=0.1)(hid)\n",
    "\n",
    "  hid = Flatten()(hid)\n",
    "  \n",
    "  merged_layer = Concatenate()([hid, condition_layer])\n",
    "  hid = Dense(512, activation='relu')(merged_layer)\n",
    "  #hid = Dropout(0.4)(hid)\n",
    "  out = Dense(1, activation='sigmoid')(hid)\n",
    "\n",
    "  model = Model(inputs=[input_layer, condition_layer], outputs=out)\n",
    "  \n",
    "  print('discriminator_model')\n",
    "  model.summary()\n",
    "\n",
    "  return model, out\n",
    "\n",
    "from keras.preprocessing import image\n",
    "\n",
    "def one_hot_encode(y):\n",
    "  z = np.zeros((len(y), 10))\n",
    "  idx = np.arange(len(y))\n",
    "  z[idx, y] = 1\n",
    "  return z\n",
    "\n",
    "def generate_noise(n_samples, noise_dim):\n",
    "  X = np.random.normal(0, 1, size=(n_samples, noise_dim))\n",
    "  return X\n",
    "\n",
    "def generate_random_labels(n):\n",
    "  y = np.random.choice(10, n)\n",
    "  y = one_hot_encode(y)\n",
    "  return y\n",
    "\n",
    "tags = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "  \n",
    "def show_samples(batchidx):\n",
    "  fig, axs = plt.subplots(5, 6, figsize=(10,6))\n",
    "  plt.subplots_adjust(hspace=0.3, wspace=0.1)\n",
    "  #fig, axs = plt.subplots(5, 6)\n",
    "  #fig.tight_layout()\n",
    "  for classlabel in range(10):\n",
    "    row = int(classlabel / 2)\n",
    "    coloffset = (classlabel % 2) * 3\n",
    "    lbls = one_hot_encode([classlabel] * 3)\n",
    "    noise = generate_noise(3, 100)\n",
    "    gen_imgs = generator.predict([noise, lbls])\n",
    "\n",
    "    for i in range(3):\n",
    "        # Dont scale the images back, let keras handle it\n",
    "        img = image.array_to_img(gen_imgs[i], scale=True)\n",
    "        axs[row,i+coloffset].imshow(img)\n",
    "        axs[row,i+coloffset].axis('off')\n",
    "#         if i ==1:\n",
    "#           axs[row,i+coloffset].set_title(tags[classlabel])\n",
    "  plt.show()\n",
    "  plt.close()\n",
    "\n",
    "# GAN creation\n",
    "img_input = Input(shape=(32,32,3))\n",
    "disc_condition_input = Input(shape=(10,))\n",
    "\n",
    "discriminator, disc_out = get_discriminator(img_input, disc_condition_input)\n",
    "discriminator.compile(optimizer=Adam(0.0002, 0.5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "discriminator.trainable = False\n",
    "\n",
    "noise_input = Input(shape=(100,))\n",
    "gen_condition_input = Input(shape=(10,))\n",
    "generator, gen_out = get_generator(noise_input, gen_condition_input)\n",
    "\n",
    "gan_input = Input(shape=(100,))\n",
    "x = generator([gan_input, gen_condition_input])\n",
    "gan_out = discriminator([x, disc_condition_input])\n",
    "gan = Model(inputs=[gan_input, gen_condition_input, disc_condition_input], output=gan_out)\n",
    "print('gan_model')\n",
    "gan.summary()\n",
    "\n",
    "gan.compile(optimizer=Adam(0.0002, 0.5), loss='binary_crossentropy')\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "?\n",
    "# # Get training images\n",
    "(X_train, y_train), (X_test, _) = cifar10.load_data()\n",
    "?\n",
    "# Normalize data\n",
    "X_train = (X_train - 127.5) / 127.5\n",
    "?\n",
    "# 1hot encode labels\n",
    "y_train = one_hot_encode(y_train[:,0])\n",
    "?\n",
    "print (\"Training shape: {}\".format(X_train.shape))\n",
    " \n",
    "num_batches = int(X_train.shape[0]/BATCH_SIZE)\n",
    "?\n",
    "# Array to store samples for experience replay\n",
    "exp_replay = []\n",
    "\n",
    "N_EPOCHS = 5000\n",
    "D_loss=np.zeros((N_EPOCHS,1))\n",
    "G_loss=np.zeros((N_EPOCHS,1))\n",
    "x=np.arange(0,N_EPOCHS)\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "  cum_d_loss = 0.\n",
    "  cum_g_loss = 0.\n",
    "  \n",
    "  for batch_idx in range(num_batches):\n",
    "    # Get the next set of real images to be used in this iteration\n",
    "    images = X_train[batch_idx*BATCH_SIZE : (batch_idx+1)*BATCH_SIZE]\n",
    "    labels = y_train[batch_idx*BATCH_SIZE : (batch_idx+1)*BATCH_SIZE]\n",
    "\n",
    "    noise_data = generate_noise(BATCH_SIZE, 100)\n",
    "    random_labels = generate_random_labels(BATCH_SIZE)\n",
    "    # We use same labels for generated images as in the real training batch\n",
    "    generated_images = generator.predict([noise_data, labels])\n",
    "\n",
    "    # Train on soft targets (add noise to targets as well)\n",
    "    noise_prop = 0.05 # Randomly flip 5% of targets\n",
    "    \n",
    "    # Prepare labels for real data\n",
    "    true_labels = np.zeros((BATCH_SIZE, 1)) + np.random.uniform(low=0.0, high=0.1, size=(BATCH_SIZE, 1))\n",
    "    flipped_idx = np.random.choice(np.arange(len(true_labels)), size=int(noise_prop*len(true_labels)))\n",
    "    true_labels[flipped_idx] = 1 - true_labels[flipped_idx]\n",
    "    \n",
    "    # Train discriminator on real data\n",
    "    d_loss_true = discriminator.train_on_batch([images, labels], true_labels)\n",
    "\n",
    "    # Prepare labels for generated data\n",
    "    gene_labels = np.ones((BATCH_SIZE, 1)) - np.random.uniform(low=0.0, high=0.1, size=(BATCH_SIZE, 1))\n",
    "    flipped_idx = np.random.choice(np.arange(len(gene_labels)), size=int(noise_prop*len(gene_labels)))\n",
    "    gene_labels[flipped_idx] = 1 - gene_labels[flipped_idx]\n",
    "    \n",
    "    # Train discriminator on generated data\n",
    "    d_loss_gene = discriminator.train_on_batch([generated_images, labels], gene_labels)\n",
    "    \n",
    "    # Store a random point for experience replay\n",
    "    r_idx = np.random.randint(BATCH_SIZE)\n",
    "    exp_replay.append([generated_images[r_idx], labels[r_idx], gene_labels[r_idx]])\n",
    "    \n",
    "    #If we have enough points, do experience replay\n",
    "    if len(exp_replay) == BATCH_SIZE:\n",
    "      generated_images = np.array([p[0] for p in exp_replay])\n",
    "      labels = np.array([p[1] for p in exp_replay])\n",
    "      gene_labels = np.array([p[2] for p in exp_replay])\n",
    "      expprep_loss_gene = discriminator.train_on_batch([generated_images, labels], gene_labels)\n",
    "      exp_replay = []\n",
    "      break\n",
    "    \n",
    "    d_loss = 0.5 * np.add(d_loss_true, d_loss_gene)\n",
    "    cum_d_loss += d_loss\n",
    "\n",
    "    # Train generator\n",
    "    noise_data = generate_noise(BATCH_SIZE, 100)\n",
    "    random_labels = generate_random_labels(BATCH_SIZE)\n",
    "    g_loss = gan.train_on_batch([noise_data, random_labels, random_labels], np.zeros((BATCH_SIZE, 1)))\n",
    "    cum_g_loss += g_loss\n",
    "    \n",
    "\n",
    "  \n",
    "  if epoch%20==0:\n",
    "    print('\\tEpoch: {}, Generator Loss: {}, Discriminator Loss: {}'.format(epoch+1, cum_g_loss/num_batches, cum_d_loss/num_batches))\n",
    "    show_samples(\"epoch\" + str(epoch))\n",
    "  \n",
    "  s=cum_d_loss/num_batches\n",
    "  a=s[0]\n",
    "  D_loss[epoch]= a\n",
    "  G_loss[epoch]= cum_g_loss/num_batches\n",
    "    \n",
    "\n",
    "plt.plot(x,D_loss)\n",
    "plt.plot(x,G_loss)\n",
    "#plt.set_title('Model Loss')\n",
    "#plt.set_ylabel('Loss')\n",
    "#plt.set_xlabel('Epoch')\n",
    "plt.legend(['discriminator', 'generator'], loc='best')\n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
